{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setup:\n",
    "\n",
    "Setup for the docs bot is simple. We are going to run this llama 13b lora on the Oogabooga text generation Web UI. Simply run all, and click the shared link at the bottom of the page to access the GUI."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install -U datasets transformers tokenizers pydantic auto_gptq gradio\n"
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T18:20:41.876419Z",
     "iopub.status.busy": "2023-07-19T18:20:41.875739Z",
     "iopub.status.idle": "2023-07-19T18:20:46.015699Z",
     "shell.execute_reply": "2023-07-19T18:20:46.015015Z",
     "shell.execute_reply.started": "2023-07-19T18:20:41.876393Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get the model checkpoint files from HuggingFace"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "!python download-model.py TheBloke/Llama-2-7B-GPTQ"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading the model to models/TheBloke_Llama-2-13B-chat-GPTQ\n",
      "100%|██████████████████████████████████████████████████| 20.3k /20.3k  28.5MiB/s\n",
      "100%|██████████████████████████████████████████████████| 4.77k /4.77k  30.5MiB/s\n",
      "100%|██████████████████████████████████████████████████| 543   /543    3.41MiB/s\n",
      "100%|███████████████████████████████████████████████████| 132   /132    545kiB/s\n",
      "100%|███████████████████████████████████████████████████| 7.26G /7.26G  366MiB/s\n",
      "100%|██████████████████████████████████████████████████| 185   /185    1.17MiB/s\n",
      "100%|██████████████████████████████████████████████████| 411   /411    2.24MiB/s\n",
      "100%|██████████████████████████████████████████████████| 1.84M /1.84M  38.7MiB/s\n",
      "100%|███████████████████████████████████████████████████| 500k  /500k   626MiB/s\n",
      "100%|██████████████████████████████████████████████████| 727   /727    3.63MiB/s\n"
     ]
    }
   ],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T18:20:04.755552Z",
     "iopub.status.busy": "2023-07-19T18:20:04.755400Z",
     "iopub.status.idle": "2023-07-19T18:24:49.698191Z",
     "shell.execute_reply": "2023-07-19T18:24:49.697556Z",
     "shell.execute_reply.started": "2023-07-19T18:24:28.541131Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Web UI\n",
    "\n",
    "Accessing the docs bot is simple:\n",
    "\n",
    "1. Make sure the setup above is complete. If it is, you should have a folder in the models directory named `huggyllama_llama-7b`, the folder `pspace-13b` in the loras directory, and have installed all the required packages\n",
    "\n",
    "2. Run the cell below to get the shared link to the GUI.\n",
    "\n",
    "3. Type a Paperspace related question into the \"Input\" box, and hit \"Generate\"\n",
    "\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python server.py --share --model TheBloke_Llama-2-13B-chat-GPTQ --load-in-8bit --bf16 --auto-devices "
   ],
   "outputs": [],
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T23:23:58.339472Z",
     "iopub.status.busy": "2023-07-19T23:23:58.339198Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}